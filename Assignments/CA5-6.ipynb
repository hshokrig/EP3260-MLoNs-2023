{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import resource\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import cvxpy\n",
    "from multiprocessing import Process, Pipe\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Load data here:\n",
    "\n",
    "# #Split dataset to 10 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- SVM Classificaton ----------------------------------\n",
    "\n",
    "#multiclass SVM classification loss and its gradient\n",
    "def SVM_classify(x, w):\n",
    "    return np.argmax(np.matmul(x.T, w), axis = 1).reshape(x.shape[1], 1)\n",
    "\n",
    "def SVM_cost(x, y, w):\n",
    "    svm_cost = (np.matmul(x.T, w) - np.repeat(np.multiply(x.T, w[:,y.reshape(y.shape[0])].T).sum(axis = 1).reshape(y.shape[0], 1), 10, axis = 1) + delta).clip(min = 0)\n",
    "    svm_cost[(range(y.shape[0]), y.reshape(y.shape[0]))] = 0\n",
    "    return svm_cost\n",
    "\n",
    "def regulated_total_cost(x, y, w, lambda_):\n",
    "    return SVM_cost(x, y, w).sum() + lambda_/2 * np.linalg.norm(w)**2\n",
    "\n",
    "def SVM_cost_grad(x, y, w, lambda_):\n",
    "    svm_cost = SVM_cost(x, y, w)\n",
    "    svm_cost[svm_cost > 0] = 1\n",
    "    svm_cost_grad_coef = svm_cost\n",
    "    svm_cost_grad_coef[(range(y.shape[0]), y.reshape(y.shape[0]))] -= svm_cost.sum(axis=1)\n",
    "    svm_cost_grad = np.zeros((x.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        svm_cost_grad[:, i] = np.multiply(svm_cost_grad_coef[:, i].reshape(svm_cost_grad_coef.shape[0], 1), x.T).sum(axis = 0)\n",
    "    return svm_cost_grad\n",
    "\n",
    "\n",
    "#binary SVM classification one-vs-rest loss and its gradient\n",
    "def bin_SVM_classify(x, w):\n",
    "    scores = np.matmul(x.T, w)\n",
    "    class_votes = np.zeros(scores.shape())\n",
    "    class_votes[scores > 0] += 1\n",
    "    for i in range(10):\n",
    "        class_votes[scores[:,i] <= 0,:] += 1\n",
    "    class_votes[scores <= 0] -= 1\n",
    "    return np.argmax(class_votes, axis = 1).reshape(x.shape[1], 1)\n",
    "            \n",
    "def bin_SVM_cost(x, y, w):\n",
    "    svm_cost = - np.matmul(x.T, w)\n",
    "    svm_cost[(range(y.shape[0]), y.reshape(y.shape[0]))] *= -1\n",
    "    return (1 - svm_cost).clip(min = 0)\n",
    "\n",
    "def bin_regulated_total_cost(x, y, w, lambda_):\n",
    "    return SVM_cost(x, y, w).sum(axis = 1) + lambda_/2 * np.linalg.norm(w, axis = 1)**2\n",
    "\n",
    "def bin_SVM_cost_grad(x, y, w, lambda_):\n",
    "    svm_cost = SVM_cost(x, y, w)\n",
    "    svm_cost[svm_cost > 0] = 1\n",
    "    svm_cost_grad = np.zeros((x.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        svm_cost_yi = svm_cost[:, i]\n",
    "        svm_cost_yi[y == i] *= -1\n",
    "        svm_cost_grad[:, i] = np.multiply(x, svm_cost_yi.T).sum(axis = 0) + lambda_ * w[:, i]\n",
    "    return svm_cost_grad\n",
    "\n",
    "\n",
    "\n",
    "#plot loss vs signaling at master node\n",
    "def plot_master_loss(k):\n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, len(curves[k]))))\n",
    "    legends = []\n",
    "    for legend in curves[k].keys():\n",
    "        w_k, errs = curves[k][legend]\n",
    "        T, loss = zip(*errs)\n",
    "        plt.plot(T, loss, color = next(colors))\n",
    "        legends.append(legend)\n",
    "    plt.xlabel(\"Signaling (T)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.title('Convergence vs signaling (%s)' % k)\n",
    "    if len(legends) > 1:\n",
    "        plt.legend(legends)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#plot greyscale plots of computed models of each digit\n",
    "def plot_w_ks(w_k):\n",
    "    fig, axarr = plt.subplots(nrows=2, ncols=5)\n",
    "    for i in range(10):\n",
    "        axarr[i / 5, i % 5].imshow(w_k[:28*28,i].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "        axarr[i / 5, i % 5].set_title(\"Digit: {}\".format(i))\n",
    "        axarr[i / 5, i % 5].set_xticks([])\n",
    "        axarr[i / 5, i % 5].set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#plot average agent loss vs signaling and the region between min and max loss\n",
    "def plot_agents_loss(k):\n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, len(curves[k].keys()))))\n",
    "    legends = []\n",
    "    for legend in curves[k].keys():\n",
    "        w_k, errs = curves[k][legend]\n",
    "        color = next(colors)\n",
    "        T, losses = zip(*errs)\n",
    "        legends.append(legend)\n",
    "        losses = zip(*losses)\n",
    "        mean = np.array(losses).mean(axis = 0)\n",
    "        lmin = np.array(losses).min(axis = 0)\n",
    "        lmax = np.array(losses).max(axis = 0)\n",
    "        var = np.array(losses).var(axis = 0)\n",
    "        plt.plot(T, mean, color = color)\n",
    "        plt.fill_between(T, lmin, lmax,facecolor = color, alpha=0.2, edgecolor='none')\n",
    "        #plt.fill_between(T, mean - var, mean + var,facecolor = color, alpha=0.2, edgecolor='none')\n",
    "    plt.xlabel(\"Signaling (T)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.yscale(\"log\")\n",
    "    if len(legends) > 1:\n",
    "        plt.legend(legends)\n",
    "    plt.title('Convergence vs signaling (%s)' % k)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #single agent GD SVM\n",
    "def gradient_descent(x, y, w_k, lambda_, a_k, max_iter):\n",
    "    L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "    errs = []\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        grad_w = SVM_cost_grad(x, y, w_k, lambda_)\n",
    "        w_k = w_k - a_k * grad_w\n",
    "        L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "        errs.append(L1)\n",
    "    return w_k.reshape(x.shape[0],10), errs \n",
    "\n",
    "\n",
    "\n",
    "#single agent binary GD SVM\n",
    "def bin_gradient_descent(x, y, w_k, lambda_, a_k, max_iter):\n",
    "    L1 = np.linalg.norm(bin_regulated_total_cost(x, y, w_k, lambda_))\n",
    "    errs = []\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        grad_w = bin_SVM_cost_grad(x, y, w_k, lambda_)\n",
    "        w_k = w_k - a_k * grad_w\n",
    "        L1 = np.linalg.norm(regulated_total_cost(x, y, w_k, lambda_))\n",
    "        errs.append(L1)\n",
    "    return w_k.reshape(x.shape[0],10), errs \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CA5---------------------------- Part (a)------------------------------------------------------------------------------\n",
    "\n",
    "#Each worker computes 10 svm loss gradients based w.r.t w for 10 classes for its partition of data adding noise with probability p :\n",
    "def decentralized_gradient_descent_worker(x, y, w_k, lambda_):\n",
    "    \n",
    "    ## compute the gradients ----------------------------------------\n",
    "   \n",
    "    return #output \n",
    "\n",
    "\n",
    "def decentralized_gradient_descent_worker_(args):\n",
    "    return decentralized_gradient_descent_worker(*args)\n",
    "\n",
    "\n",
    "#Master updates the gradient w.r.t w_k for each class\n",
    "def decentralized_gradient_descent_master(w_k, a_k, grad_w, x, y, lambda_):\n",
    "    ## compute w_k --------------------------------------------------\n",
    "   \n",
    "    return 10, w_k\n",
    "\n",
    "\n",
    "#DGD\n",
    "def decentralized_gradient_descent(x, y, workers_x, workers_y, lambda_, a_k, max_iter, master, worker_, compress = None):\n",
    "    #each class has a w_k\n",
    "    w_k = np.random.rand(x.shape[0], 10)\n",
    "    T = 0\n",
    "    errs = []\n",
    "    L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        #copy w_k of classes for workers\n",
    "        if compress is not None:\n",
    "            w_k = compress(w_k)\n",
    "        workers_w_k = np.array(10 * [w_k]).reshape(10, x.shape[0], 10)\n",
    "        \n",
    "        #each worker gets a copy of w_ks for 10 classes, and its share of data and returns 10 loss gradients, one per class\n",
    "        zipped = zip(workers_x, workers_y, workers_w_k, 10 * [lambda_])\n",
    "        \n",
    "        pool = ThreadPool(10)\n",
    "        results = pool.map(worker_, zipped)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        ## complete signal, grad_w ---------------------\n",
    "        signals, grad_w = ##output  \n",
    "        grad_w = np.array(grad_w)\n",
    "        \n",
    "        ## calculate T ---------------------------------\n",
    "        T +=  #output \n",
    "        if compress is not None:\n",
    "            for i in range(10):\n",
    "                grad_w[i] = compress(grad_w[i])\n",
    "                \n",
    "        #master updates w_ks\n",
    "        \n",
    "        ## complete signal, grad_w ---------------------\n",
    "        signals, w_k = #output \n",
    "        ## calculate T\n",
    "        T +=  #output \n",
    "        \n",
    "        L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "        errs.append((T, L1))\n",
    "    print(\"Final loss = %.3f and gradient norm = %.3f\" %(L1, np.linalg.norm(grad_w)))\n",
    "    return w_k.reshape(x.shape[0],10), errs\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------Run DGD ------------------------------------------------\n",
    "\n",
    "\n",
    "## Complete w_k and errs ---------------------------------------\n",
    "w_k, errs = # output  \n",
    "\n",
    "\n",
    "y_predict =  SVM_classify(x_train, w_k)\n",
    "print(\"Training classification error: %.3f\" % nmae(y_train, y_predict))\n",
    "\n",
    "\n",
    "y_predict = SVM_classify(x_test, w_k)\n",
    "\n",
    "print(\"Test classification error: %.3f\" % nmae(y_test, y_predict))\n",
    "\n",
    "curves['decentralized gradient descent'] = {'no compression':(w_k, errs)}\n",
    "\n",
    "\n",
    "# ------------------------------plot greyscale models and master loss for DGD ----------------\n",
    "# complete the plot \n",
    "plot_w_ks(curves['decentralized gradient descent']['no compression'][0])\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CA5---------------------------- Part (b)------------------------------------------------------------------------------\n",
    "\n",
    "# Complete communication graph -----------------------\n",
    "A = ## output\n",
    "# ----------------------------------------------------\n",
    "\n",
    "#agent process for decentralized subgradient method (DSM)\n",
    "\n",
    "def decentralized_subgradient_method_agent(Ai, x, y, w_k, lambda_, a_k):\n",
    "    \n",
    "# complete the following--------------------------------------   \n",
    "    bar_w_k = # output \n",
    "    grad_w = # output \n",
    "    bar_w_k = # output \n",
    "    return # output \n",
    "\n",
    "\n",
    "def decentralized_subgradient_method_agent_(args):\n",
    "    return decentralized_subgradient_method_agent(*args)\n",
    "\n",
    "# complete the following for ADMM --------------------------------------   \n",
    "\n",
    "A_ADMM = A - #complete it \n",
    "\n",
    "#agent process for ADMM\n",
    "# complete the following--------------------------------------   \n",
    "def admm_over_network_agent(Ai, x, y, w_k, lambda_, a_k):\n",
    "    bar_w_k = # output \n",
    "    grad_w = # output \n",
    "    bar_w_k = # output \n",
    "    return # output \n",
    "\n",
    "def admm_over_network_agent_(args):\n",
    "    return admm_over_network_agent(*args)\n",
    "\n",
    "\n",
    "#decentralized optimization over network\n",
    "def decentralized_method(x, y, workers_x, workers_y, lambda_, a_k, max_iter, A, agent_, compress = None):\n",
    "    #each class has a w_k\n",
    "    w_k = np.random.rand(10, x.shape[0], 10)\n",
    "    T = 0\n",
    "    errs = []\n",
    "    ## Calculate L1 ------------------------------------------------\n",
    "    L1 = # output \n",
    "   \n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        #transmit w_ks and \n",
    "        workers_w_k = np.array(10 * [w_k]).reshape(10, 10, x.shape[0], 10)\n",
    "        if compress is not None:\n",
    "            compressed_w_k = np.zeros((10, x.shape[0], 10))\n",
    "            for i in range(10):\n",
    "                compressed_w_k[i] = compress(w_k[i])\n",
    "            workers_w_k = np.array(10 * [compressed_w_k]).reshape(10, 10, x.shape[0], 10)\n",
    "            for i in range(10):\n",
    "                workers_w_k[i, i] = w_k[i]\n",
    "        #each agent gets a copy of w_ks of neighbors, and its share of data and returns 10 loss gradients, one per class\n",
    "        zipped = zip(A, workers_x, workers_y, workers_w_k, 10 * [lambda_], 10 * [a_k])\n",
    "        \n",
    "        pool = ThreadPool(10)\n",
    "        results = pool.map(agent_, zipped)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # Complete signals, w_k, grad_w ------------------------------ \n",
    "        signals, w_k, grad_w = #output \n",
    "        grad_w = np.array(grad_w)\n",
    "        w_k = np.array(w_k)\n",
    "        \n",
    "         # Complete T ------------------------------------------------\n",
    "        T += #output \n",
    "        \n",
    "         # Complete L1 ----------------------------------------------\n",
    "        L1 = #output \n",
    "        \n",
    "        L1s = []\n",
    "        for i in range(10):\n",
    "            L1s.append(regulated_total_cost(x, y, w_k[i], lambda_))\n",
    "        errs.append((T, L1s))\n",
    "    print(\"Final loss = %.3f and gradient norm = %.3f\" %(L1, np.linalg.norm(grad_w.mean(axis = 0))))\n",
    "    return w_k.mean(axis = 0).reshape(x.shape[0],10), errs\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------- Run DSM ------------------------------------------------------\n",
    "print(datetime.now())\n",
    "\n",
    " # complete w_k, errs -------------------------\n",
    "w_k, errs = #output \n",
    "print(datetime.now())\n",
    "\n",
    "y_predict = SVM_classify(x_train, w_k)\n",
    "print(\"Training classification error: %.3f\" % nmae(y_train, y_predict))\n",
    "y_predict = SVM_classify(x_test, w_k)\n",
    "print(\"Test classification error: %.3f\" % nmae(y_test, y_predict))\n",
    "\n",
    "curves['decentralized subgradient method'] = {'no compression':(w_k, errs)}\n",
    "\n",
    "# ------------------------------------------Run ADMM ----------------------------------------------------\n",
    "print(datetime.now())\n",
    "# complete w_k, errs -------------------------\n",
    "w_k, errs = #output \n",
    "print(datetime.now())\n",
    "\n",
    "y_predict = SVM_classify(x_train, w_k)\n",
    "print(\"Training accuracy: %.3f\" % nmae(y_train, y_predict))\n",
    "y_predict = SVM_classify(x_test, w_k)\n",
    "print(\"Test accuracy: %.3f\" % nmae(y_test, y_predict))\n",
    "\n",
    "\n",
    "curves['ADMM over network'] = {'no compression':(w_k, errs)}\n",
    "\n",
    "\n",
    "#plot greyscale models and average agent loss for DSM\n",
    "plot_w_ks(curves['decentralized subgradient method']['no compression'][0])\n",
    "\n",
    "plot_agents_loss('decentralized subgradient method')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------plot greyscale models and average agent loss for ADMM -----------------------\n",
    "\n",
    "\n",
    "# complete the plot \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CA5---------------------------- Part (c)------------------------------------------------------------------------------\n",
    "\n",
    "# Propose your approach here -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CA6---------------------------- Part (a)------------------------------------------------------------------------------\n",
    "\n",
    "# Complete Q1 and Q2 compression functions here -----------------------\n",
    "\n",
    "# Repeat part a-b from CA5 using Q1 and Q2 compression functions here -----------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
