{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2481dde2-4dfa-48aa-ab6f-6dd4e4715cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238905bc-364d-4b4d-9db2-c166ca8dff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767dc48-9d59-43a7-af8b-9fb5f3cafa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369a80e-750b-4d1d-ad58-6eb130afe72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1), derivative=False)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2), derivative=False)\n",
    "    layer_3 = np.dot(layer_2, w_3)\n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "    \n",
    "# Fill in backpropagation    \n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3):\n",
    "    error = layer_3 - y\n",
    "    \n",
    "    # derivative of the error with respect to w_3\n",
    "    layer_3_delta = np.dot(layer_2.T, error)\n",
    "    \n",
    "    # derivative of the error with respect to  w_2\n",
    "    d_layer_2 = np.dot(error, w_3.T) * sigmoid(layer_2, derivative=True)\n",
    "    layer_2_delta = np.dot(layer_1.T, d_layer_2)\n",
    "    \n",
    "    # derivative of the error with respect to w_1\n",
    "    d_layer_1 = np.dot(d_layer_2, w_2.T) * sigmoid(layer_1, derivative=True)\n",
    "    layer_1_delta = np.dot(layer_0.T, d_layer_1)\n",
    "\n",
    "    return layer_1_delta, layer_2_delta, layer_3_delta\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "    regularization = (lmbda) * ((np.linalg.norm(w_1)** 2) +( np.linalg.norm(w_2) ** 2) + (np.linalg.norm(w_3)** 2))\n",
    "    return regularization + np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    for i in range(iterations):\n",
    "        # Forward pass\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "        # Backward pass\n",
    "        d_w_1, d_w_2, d_w_3 = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3)\n",
    "        \n",
    "        # Regularization\n",
    "        d_w_1 += lmbda * w_1\n",
    "        d_w_2 += lmbda * w_2\n",
    "        d_w_3 += lmbda * w_3\n",
    "        \n",
    "        # Update weights\n",
    "        w_1 -= learning_rate * d_w_1\n",
    "        w_2 -= learning_rate * d_w_2\n",
    "        w_3 -= learning_rate * d_w_3\n",
    "    \n",
    "\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations, noise):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41aff95-1fd3-4637-bf0a-7d4c1270d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1., -1., ..., -1., -1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bd920-99b4-4a6e-a9ad-f7602bd1bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0.01, dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=32, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=100)\n",
    "\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "\n",
    "    {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": None\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6dc72b-91c6-4004-93c2-9d6cd16a5e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3547870537.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/l9/nl6ry63s0m3_35qt7mrjv0fh0000gn/T/ipykernel_21954/3547870537.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    parser.add_argument('--lambda', type=float, default=, dest='lmbda')\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=, dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=)\n",
    "parser.add_argument('--iterations', type=int, default=)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8e974-ae5b-45a5-87b2-c98b1c4f537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a355a-aa68-4de4-9dff-ea98aa87889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
