{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31de1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "from multiprocessing import Process, Pipe\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2a98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "# Setup train and test splits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Making a copy before flattening for the next code-segment which displays images\n",
    "x_train_drawing = x_train\n",
    "\n",
    "image_size = 784 # 28 x 28\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size) /255\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size)/255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "#Split dataset to num_worker workers\n",
    "def split_workers(X_data, y_data, num_worker):\n",
    "    # Split into 10 subdatasets for 10 workers\n",
    "    data_X_list=[]\n",
    "    data_y_list=[]\n",
    "    num_data = len(y_data)\n",
    "    num_per_data = num_data // num_worker\n",
    "    for i_th in range(num_worker):\n",
    "        j = num_per_data * (i_th + 1)\n",
    "        i = i_th*num_per_data\n",
    "        x_data_worker = X_data[i:j]\n",
    "        y_data_worker = y_data[i:j]\n",
    "        data_X_list.append(x_data_worker)\n",
    "        data_y_list.append(y_data_worker)\n",
    "\n",
    "    return data_X_list, data_y_list\n",
    "\n",
    "# Split into 10 subdatasets for 10 workers\n",
    "num_worker=10\n",
    "data_X_list, data_y_list = split_workers(x_train, y_train, num_worker)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae7e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- SVM Classificaton ----------------------------------\n",
    "\n",
    "#multiclass SVM classification loss and its gradient\n",
    "def SVM_classify(x, w):\n",
    "    return np.argmax(np.matmul(x.T, w), axis = 1).reshape(x.shape[1], 1)\n",
    "\n",
    "def SVM_cost(x, y, w):\n",
    "    svm_cost = (np.matmul(x.T, w) - np.repeat(np.multiply(x.T, w[:,y.reshape(y.shape[0])].T).sum(axis = 1).reshape(y.shape[0], 1), 10, axis = 1) + delta).clip(min = 0)\n",
    "    svm_cost[(range(y.shape[0]), y.reshape(y.shape[0]))] = 0\n",
    "    return svm_cost\n",
    "\n",
    "def regulated_total_cost(x, y, w, lambda_):\n",
    "    return SVM_cost(x, y, w).sum() + lambda_/2 * np.linalg.norm(w)**2\n",
    "\n",
    "def SVM_cost_grad(x, y, w, lambda_):\n",
    "    svm_cost = SVM_cost(x, y, w)\n",
    "    svm_cost[svm_cost > 0] = 1\n",
    "    svm_cost_grad_coef = svm_cost\n",
    "    svm_cost_grad_coef[(range(y.shape[0]), y.reshape(y.shape[0]))] -= svm_cost.sum(axis=1)\n",
    "    svm_cost_grad = np.zeros((x.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        svm_cost_grad[:, i] = np.multiply(svm_cost_grad_coef[:, i].reshape(svm_cost_grad_coef.shape[0], 1), x.T).sum(axis = 0)\n",
    "    return svm_cost_grad\n",
    "\n",
    "\n",
    "#binary SVM classification one-vs-rest loss and its gradient\n",
    "def bin_SVM_classify(x, w):\n",
    "    scores = np.matmul(x.T, w)\n",
    "    class_votes = np.zeros(scores.shape())\n",
    "    class_votes[scores > 0] += 1\n",
    "    for i in range(10):\n",
    "        class_votes[scores[:,i] <= 0,:] += 1\n",
    "    class_votes[scores <= 0] -= 1\n",
    "    return np.argmax(class_votes, axis = 1).reshape(x.shape[1], 1)\n",
    "            \n",
    "def bin_SVM_cost(x, y, w):\n",
    "    svm_cost = - np.matmul(x.T, w)\n",
    "    svm_cost[(range(y.shape[0]), y.reshape(y.shape[0]))] *= -1\n",
    "    return (1 - svm_cost).clip(min = 0)\n",
    "\n",
    "def bin_regulated_total_cost(x, y, w, lambda_):\n",
    "    return SVM_cost(x, y, w).sum(axis = 1) + lambda_/2 * np.linalg.norm(w, axis = 1)**2\n",
    "\n",
    "def bin_SVM_cost_grad(x, y, w, lambda_):\n",
    "    svm_cost = SVM_cost(x, y, w)\n",
    "    svm_cost[svm_cost > 0] = 1\n",
    "    svm_cost_grad = np.zeros((x.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        svm_cost_yi = svm_cost[:, i]\n",
    "        svm_cost_yi[y == i] *= -1\n",
    "        svm_cost_grad[:, i] = np.multiply(x, svm_cost_yi.T).sum(axis = 0) + lambda_ * w[:, i]\n",
    "    return svm_cost_grad\n",
    "\n",
    "\n",
    "\n",
    "#plot loss vs signaling at master node\n",
    "def plot_master_loss(k):\n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, len(curves[k]))))\n",
    "    legends = []\n",
    "    for legend in curves[k].keys():\n",
    "        w_k, errs = curves[k][legend]\n",
    "        T, loss = zip(*errs)\n",
    "        plt.plot(T, loss, color = next(colors))\n",
    "        legends.append(legend)\n",
    "    plt.xlabel(\"Signaling (T)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.title('Convergence vs signaling (%s)' % k)\n",
    "    if len(legends) > 1:\n",
    "        plt.legend(legends)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#plot greyscale plots of computed models of each digit\n",
    "def plot_w_ks(w_k):\n",
    "    fig, axarr = plt.subplots(nrows=2, ncols=5)\n",
    "    for i in range(10):\n",
    "        axarr[i / 5, i % 5].imshow(w_k[:28*28,i].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "        axarr[i / 5, i % 5].set_title(\"Digit: {}\".format(i))\n",
    "        axarr[i / 5, i % 5].set_xticks([])\n",
    "        axarr[i / 5, i % 5].set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#plot average agent loss vs signaling and the region between min and max loss\n",
    "def plot_agents_loss(k):\n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, len(curves[k].keys()))))\n",
    "    legends = []\n",
    "    for legend in curves[k].keys():\n",
    "        w_k, errs = curves[k][legend]\n",
    "        color = next(colors)\n",
    "        T, losses = zip(*errs)\n",
    "        legends.append(legend)\n",
    "        losses = zip(*losses)\n",
    "        mean = np.array(losses).mean(axis = 0)\n",
    "        lmin = np.array(losses).min(axis = 0)\n",
    "        lmax = np.array(losses).max(axis = 0)\n",
    "        var = np.array(losses).var(axis = 0)\n",
    "        plt.plot(T, mean, color = color)\n",
    "        plt.fill_between(T, lmin, lmax,facecolor = color, alpha=0.2, edgecolor='none')\n",
    "        #plt.fill_between(T, mean - var, mean + var,facecolor = color, alpha=0.2, edgecolor='none')\n",
    "    plt.xlabel(\"Signaling (T)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.yscale(\"log\")\n",
    "    if len(legends) > 1:\n",
    "        plt.legend(legends)\n",
    "    plt.title('Convergence vs signaling (%s)' % k)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #single agent GD SVM\n",
    "def gradient_descent(x, y, w_k, lambda_, a_k, max_iter):\n",
    "    L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "    errs = []\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        grad_w = SVM_cost_grad(x, y, w_k, lambda_)\n",
    "        w_k = w_k - a_k * grad_w\n",
    "        L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "        errs.append(L1)\n",
    "    return w_k.reshape(x.shape[0],10), errs \n",
    "\n",
    "\n",
    "\n",
    "#single agent binary GD SVM\n",
    "def bin_gradient_descent(x, y, w_k, lambda_, a_k, max_iter):\n",
    "    L1 = np.linalg.norm(bin_regulated_total_cost(x, y, w_k, lambda_))\n",
    "    errs = []\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        grad_w = bin_SVM_cost_grad(x, y, w_k, lambda_)\n",
    "        w_k = w_k - a_k * grad_w\n",
    "        L1 = np.linalg.norm(regulated_total_cost(x, y, w_k, lambda_))\n",
    "        errs.append(L1)\n",
    "    return w_k.reshape(x.shape[0],10), errs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf0a380",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workers_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m compress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Complete w_k and errs ---------------------------------------\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m w_k, errs \u001b[38;5;241m=\u001b[39m decentralized_gradient_descent(x_train, y_train, \u001b[43mworkers_x\u001b[49m, workers_y, lambda_, a_k, max_iter, decentralized_gradient_descent_master, decentralized_gradient_descent_worker_, compress)\n\u001b[0;32m    101\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m SVM_classify(x_train, w_k)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining classification error: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m nmae(y_train, y_predict))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'workers_x' is not defined"
     ]
    }
   ],
   "source": [
    "## CA5---------------------------- Part (a)------------------------------------------------------------------------------\n",
    "\n",
    "#Each worker computes 10 svm loss gradients based w.r.t w for 10 classes for its partition of data adding noise with probability p :\n",
    "def decentralized_gradient_descent_worker(x, y, w_k, lambda_):\n",
    "    \n",
    "    ## compute the gradients ----------------------------------------\n",
    "    n = x.shape[1]\n",
    "    d = x.shape[0]\n",
    "    k = w_k.shape[1]\n",
    "    \n",
    "    grad_w = np.zeros((d, k))\n",
    "    \n",
    "    for c in range(k):\n",
    "        idx_c = np.where(y == c)[0]\n",
    "        x_c = x[:, idx_c]\n",
    "        y_c = np.where(y[idx_c] == c, 1, -1)\n",
    "        n_c = len(idx_c)\n",
    "        if n_c == 0:\n",
    "            continue\n",
    "        w_c = w_k[:, c]\n",
    "        z = y_c * (w_c.T @ x_c)\n",
    "        mask = np.random.binomial(1, p, size=z.shape)\n",
    "        z[mask == 1] = -z[mask == 1]\n",
    "        z = np.maximum(0, 1 - z)\n",
    "        dz = -y_c * z\n",
    "        grad_w[:, c] = (x_c @ dz.T) / n_c + 2 * lambda_ * w_c\n",
    "    \n",
    "    return grad_w\n",
    "\n",
    "\n",
    "def decentralized_gradient_descent_worker_(args):\n",
    "    return decentralized_gradient_descent_worker(*args)\n",
    "\n",
    "\n",
    "#Master updates the gradient w.r.t w_k for each class\n",
    "def decentralized_gradient_descent_master(w_k, a_k, grad_w, x, y, lambda_):\n",
    "    ## compute w_k --------------------------------------------------\n",
    "    w_k_new = np.zeros_like(w_k)\n",
    "    for c in range(w_k.shape[1]):\n",
    "        w_k_new[:, c] = w_k[:, c] - a_k * grad_w[c]\n",
    "        w_k_new[:, c] = prox_l1(w_k_new[:, c], a_k * lambda_)\n",
    "    return a_k, w_k_new\n",
    "\n",
    "\n",
    "\n",
    "#DGD\n",
    "def decentralized_gradient_descent(x, y, workers_x, workers_y, lambda_, a_k, max_iter, master, worker_, compress = None):\n",
    "    #each class has a w_k\n",
    "    w_k = np.random.rand(x.shape[0], 10)\n",
    "    T = 0\n",
    "    errs = []\n",
    "    L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "    for k in range(max_iter):\n",
    "        L = L1\n",
    "        #copy w_k of classes for workers\n",
    "        if compress is not None:\n",
    "            w_k = compress(w_k)\n",
    "        workers_w_k = np.array(10 * [w_k]).reshape(10, x.shape[0], 10)\n",
    "        \n",
    "        #each worker gets a copy of w_ks for 10 classes, and its share of data and returns 10 loss gradients, one per class\n",
    "        zipped = zip(workers_x, workers_y, workers_w_k, 10 * [lambda_])\n",
    "        \n",
    "        pool = ThreadPool(10)\n",
    "        results = pool.map(worker_, zipped)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        ## complete signal, grad_w ---------------------\n",
    "        signals, grad_w = master(w_k, a_k, np.array(results), x, y, lambda_)\n",
    "        grad_w = np.array(grad_w)\n",
    "        \n",
    "        ## calculate T ---------------------------------\n",
    "        T += len(signals)\n",
    "        if compress is not None:\n",
    "            for i in range(10):\n",
    "                grad_w[i] = compress(grad_w[i])\n",
    "                \n",
    "        #master updates w_ks\n",
    "        \n",
    "        ## complete signal, grad_w ---------------------\n",
    "        signals, w_k = master(w_k, a_k, grad_w, x, y, lambda_)\n",
    "        ## calculate T\n",
    "        T += len(signals)\n",
    "        \n",
    "        L1 = regulated_total_cost(x, y, w_k, lambda_)\n",
    "        errs.append((T, L1))\n",
    "    print(\"Final loss = %.3f and gradient norm = %.3f\" %(L1, np.linalg.norm(grad_w)))\n",
    "    return w_k.reshape(x.shape[0],10), errs\n",
    "\n",
    "#---------------------------------------------------Run DGD ------------------------------------------------\n",
    "\n",
    "## Define parameters -----------------------------\n",
    "lambda_ = 1e-2\n",
    "a_k = 0.01\n",
    "max_iter = 100\n",
    "compress = None\n",
    "\n",
    "# Complete w_k and errs ---------------------------------------\n",
    "w_k, errs = decentralized_gradient_descent(x_train, y_train, workers_x, workers_y, lambda_, a_k, max_iter, decentralized_gradient_descent_master, decentralized_gradient_descent_worker_, compress)\n",
    "\n",
    "y_predict = SVM_classify(x_train, w_k)\n",
    "print(\"Training classification error: %.3f\" % nmae(y_train, y_predict))\n",
    "\n",
    "y_predict = SVM_classify(x_test, w_k)\n",
    "print(\"Test classification error: %.3f\" % nmae(y_test, y_predict))\n",
    "\n",
    "curves['decentralized gradient descent'] = {'no compression':(w_k, errs)}\n",
    "\n",
    "# ------------------------------plot greyscale models and master loss for DGD ----------------\n",
    "# complete the plot \n",
    "plot_w_ks(curves['decentralized gradient descent']['no compression'][0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
